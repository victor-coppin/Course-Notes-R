[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science with R",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Tidyverse.html",
    "href": "Tidyverse.html",
    "title": "1  The Tidyverse",
    "section": "",
    "text": "2 Manipulating Data frames with dplyr and purrr\nThe Tidyverse can be installed with a single line of code: install.packages(“tidyverse”)\nThis command installs the nine core packages of the Tidyverse: dplyr, forcats, ggplot2, lubridate, purrr, readr, stringr, tibble, and tidyr. These are considered the core of the Tidyverse because you’ll use them in almost every analysis: - dplyr : manipulating data frames\n- forcats : provides tools for dealing with categorical variables\n- ggplot2 : producing statistical, or data, graphics\n- lubridate : makes it easier to work with dates and times in R\n- purr : working with functions and iteration in a functional programming style\n#| label: load-tidyverse #| warning = FALSE #| message = FALSE",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Tidyverse</span>"
    ]
  },
  {
    "objectID": "Tidyverse.html#tidy-data",
    "href": "Tidyverse.html#tidy-data",
    "title": "1  The Tidyverse",
    "section": "2.1 Tidy Data",
    "text": "2.1 Tidy Data\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represents a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Tidyverse</span>"
    ]
  },
  {
    "objectID": "Tidyverse.html#manipulating-data-frames",
    "href": "Tidyverse.html#manipulating-data-frames",
    "title": "1  The Tidyverse",
    "section": "2.2 Manipulating Data Frames",
    "text": "2.2 Manipulating Data Frames\n\n“The dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.”\n\n\n2.2.1 The mutate function\nThe mutate function is used to add new columns to a data frame or modify existing ones.\n\n# Add a new column 'rate' to the murders data frame\nmurders  &lt;- mutate(murders, rate = total / population * 100000)\n\nNote: to compute the rate, we used total and population columns, which are not defined in the global environment. The mutate function allows us to use the names of the columns directly.\n\n“This is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument.\nIn the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable and concise.”\n\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: the mutate function does not change the original data frame.\n\n“Although we have overwritten the original murders object, this does not change the object that is loaded with data(murders).\nIf we load the murders data again, the original will overwrite our mutated version.”\n\n\n\n2.2.2 Subsetting with filter\nThe filter function is used to subset rows based on logical conditions.\nFilter the murders data frame to include only the entries for which the murder rate is lower than 0.71.\n\n# Syntax : data, conditional statement.\nfilter(murders, rate &lt;= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\n2.2.3 Selecting columns with select\nThe select() function is used to extract specific columns from a data frame.\nIn the example below: - We create a new data frame containing only the columns state, region, and rate. - We then apply filter() to keep only the rows where the murder rate is less than or equal to 0.71.\n\nstate_region_rate_table &lt;- select(murders, state, region, rate)\nfilter(state_region_rate_table, rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\n\n\n2.2.4 Exercises\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\n\nUse the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\n\n\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\nmurders &lt;- mutate(murders, rank = rank(-rate))\nmurders %&gt;% head()\n\n       state abb region population total     rate rank\n1    Alabama  AL  South    4779736   135 2.824424   23\n2     Alaska  AK   West     710231    19 2.675186   27\n3    Arizona  AZ   West    6392017   232 3.629527   10\n4   Arkansas  AR  South    2915918    93 3.189390   17\n5 California  CA   West   37253956  1257 3.374138   14\n6   Colorado  CO   West    5029196    65 1.292453   38\n\nselect(murders, state, population) %&gt;% head()\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\n\nWe can write population rather than murders$population. The function mutate knows we are grabbing columns from murders.\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\n\nselect(murders, state, abb)\n\n                  state abb\n1               Alabama  AL\n2                Alaska  AK\n3               Arizona  AZ\n4              Arkansas  AR\n5            California  CA\n6              Colorado  CO\n7           Connecticut  CT\n8              Delaware  DE\n9  District of Columbia  DC\n10              Florida  FL\n11              Georgia  GA\n12               Hawaii  HI\n13                Idaho  ID\n14             Illinois  IL\n15              Indiana  IN\n16                 Iowa  IA\n17               Kansas  KS\n18             Kentucky  KY\n19            Louisiana  LA\n20                Maine  ME\n21             Maryland  MD\n22        Massachusetts  MA\n23             Michigan  MI\n24            Minnesota  MN\n25          Mississippi  MS\n26             Missouri  MO\n27              Montana  MT\n28             Nebraska  NE\n29               Nevada  NV\n30        New Hampshire  NH\n31           New Jersey  NJ\n32           New Mexico  NM\n33             New York  NY\n34       North Carolina  NC\n35         North Dakota  ND\n36                 Ohio  OH\n37             Oklahoma  OK\n38               Oregon  OR\n39         Pennsylvania  PA\n40         Rhode Island  RI\n41       South Carolina  SC\n42         South Dakota  SD\n43            Tennessee  TN\n44                Texas  TX\n45                 Utah  UT\n46              Vermont  VT\n47             Virginia  VA\n48           Washington  WA\n49        West Virginia  WV\n50            Wisconsin  WI\n51              Wyoming  WY\n\n\n\nUse filter to show the top 5 states with the highest murder rates.\n\n\nfilter(murders,  rank &lt;= 5)\n\n                 state abb        region population total      rate rank\n1 District of Columbia  DC         South     601723    99 16.452753    1\n2            Louisiana  LA         South    4533372   351  7.742581    2\n3             Maryland  MD         South    5773552   293  5.074866    4\n4             Missouri  MO North Central    5988927   321  5.359892    3\n5       South Carolina  SC         South    4625364   207  4.475323    5\n\n\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nNote: We can remove rows using the != operator. For example, to remove Florida, we would do this:\n\nno_florida &lt;- filter(murders, state != \"Florida\")\n\n\n# Create the new data frame without south region\nno_south &lt;- filter(murders, region != \"South\")\n# Compute how many states are not in the south\nselect(no_south, state) %&gt;% nrow()\n\n[1] 34\n\n\nThere are 34 states which are not in the south",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Tidyverse</span>"
    ]
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "2  data-exploration",
    "section": "",
    "text": "2.1 help\nShows the help page with a description of the dataset and its variables\n?mpg  # for quick help lookup\nhelp(mgp) # detailed help",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>data-exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#class",
    "href": "data-exploration.html#class",
    "title": "2  data-exploration",
    "section": "2.2 Class",
    "text": "2.2 Class\n\nclass(mpg) \n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>data-exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#str",
    "href": "data-exploration.html#str",
    "title": "2  data-exploration",
    "section": "2.3 str",
    "text": "2.3 str\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>data-exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#glimpse",
    "href": "data-exploration.html#glimpse",
    "title": "2  data-exploration",
    "section": "2.4 glimpse",
    "text": "2.4 glimpse\nglimpse() (dplyr package) provides an overview of the data set/a transposed version of the data, showing the number of observations, variable names, data types, and a sample of the data stored in each variable\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>data-exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#summary",
    "href": "data-exploration.html#summary",
    "title": "2  data-exploration",
    "section": "2.5 summary",
    "text": "2.5 summary\n\nsummary(mpg)\n\n manufacturer          model               displ            year     \n Length:234         Length:234         Min.   :1.600   Min.   :1999  \n Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  \n Mode  :character   Mode  :character   Median :3.300   Median :2004  \n                                       Mean   :3.472   Mean   :2004  \n                                       3rd Qu.:4.600   3rd Qu.:2008  \n                                       Max.   :7.000   Max.   :2008  \n      cyl           trans               drv                 cty       \n Min.   :4.000   Length:234         Length:234         Min.   : 9.00  \n 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  \n Median :6.000   Mode  :character   Mode  :character   Median :17.00  \n Mean   :5.889                                         Mean   :16.86  \n 3rd Qu.:8.000                                         3rd Qu.:19.00  \n Max.   :8.000                                         Max.   :35.00  \n      hwy             fl               class          \n Min.   :12.00   Length:234         Length:234        \n 1st Qu.:18.00   Class :character   Class :character  \n Median :24.00   Mode  :character   Mode  :character  \n Mean   :23.44                                        \n 3rd Qu.:27.00                                        \n Max.   :44.00",
    "crumbs": [
      "R Basics : Introduction to Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>data-exploration</span>"
    ]
  },
  {
    "objectID": "ggplot2-introduction.html",
    "href": "ggplot2-introduction.html",
    "title": "3  Elegant Graphics for Data Analysis",
    "section": "",
    "text": "The following content is provided from the book ggplot2: Elegant Graphics for Data Analysis written by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen.ggplot2-book.org\nggplot2 is one of the core packages of the tidyverse library, designed for producing statistical (data) graphics. This package is based on the Grammar of Graphics (Wilkinson, 2005), which allows users to compose graphs by combining independent components.\nggplot2 is designed to work iteratively, layer by layer.\n\n4 Grammar of Graphics\nCreated by Wilkinson in 2005, the Grammar of Graphics aims to “describe the fundamental features that underlie all statistical graphics.”\n\n“In brief, the grammar tells us that a graphic maps the data to the aesthetic attributes of geometric objects. The plot may also include statistical transformations of the data and information about the plot’s coordinate system. Facetting can be used to plot for different subsets of the data. The combination of these independent components is what makes up a graphic.” (Wickham, Navarro, & Pedersen, 2023)\n\nResources - The built-in documentation\n- cheatsheets",
    "crumbs": [
      "ggplot2: Elegant Graphics for Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elegant Graphics for Data Analysis</span>"
    ]
  },
  {
    "objectID": "Fist-Steps-in-ggplot2.html",
    "href": "Fist-Steps-in-ggplot2.html",
    "title": "4  Key components",
    "section": "",
    "text": "4.1 Aesthetic attributes : colour, size, shape\nThe data set : “Fuel economy data”\nThe mpg dataset is introduced in @wickham2023ggplot2 for early plotting examples. It includes information about the fuel economy of popular car models in 1999 and 2008, collected by the US Environmental Protection Agency.\nSee Chapter 2 to have a look at the different ways to discover this dataset.\nThree key components:\nFor instance  The code bellow call the data mpg and the aesthetic aes() that link :\n- x to displ (engine displacement, in litres)\n- y to hwy (highway miles per gallon)\nThen a layer geom_point() is added on with + to create scatterplots.\nTips &gt;“Almost every plot maps a variable to x and y, so naming these aesthetics is tedious, so the first two unnamed arguments to aes() will be mapped to x and y. This means that the following code is identical to the example above:”[@wickham2023ggplot2]\nWe can add options or parameters to the aes() functions : aes(disply, hwy, colour = class) : map the variable class for each (x,y) to a colour aes(disply, hwy, size = cyl) : geom_points size will be mapped to the cyl variable. aes(disply, hwy, shape = drv) : the shape aesthetic controls the symbols of points\n# First plot with shape aes \nggplot(mpg, aes(displ,hwy, shape = class)) +\n    geom_point()\n# Second plot with colour aes \nggplot(mpg, aes(displ,hwy, colour = class)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n(a) shape\n\n\n\n\n\n\n\n\n\n\n\n(b) colour\n\n\n\n\n\n\nFigure 4.1: Aesthetic attibutes\nThis gives each point a unique shape or colour corresponding to its class. ggplot2 converting data into aesthetic with a scale : there is one scale for each aesthetic mapping in a plot. The scale is also responsible for creating a guide, an axis or a legend.\nTo set an aesthetic to a fixed value, without scaling it, do so in the individual layer outside of aes().\nggplot(mpg, aes(displ,hwy,)) + geom_point(colour= \"blue\")",
    "crumbs": [
      "ggplot2: Elegant Graphics for Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Key components</span>"
    ]
  },
  {
    "objectID": "FSML-MQE.html",
    "href": "FSML-MQE.html",
    "title": "6  Mean Quadratic Error",
    "section": "",
    "text": "The MQE is a measure of how close the estimator is to the true parameter value.\nTo compare estimator we can compute the mean quadratic Eroor, denoted by MQE : \\[ \\Large\\text{MQE}(\\hat{\\theta}_{n}) = \\mathrm{Var}\\left(\\hat{\\theta}_n\\right) + \\left(b_{\\theta}\\left(\\hat{\\theta}_n\\right)\\right)^2\\]\nwhere \\(\\beta_{\\theta}(\\hat{\\theta}_{n}) = \\mathbb{E}\\left[\\hat{\\theta}_n\\right] - \\large\\theta\\) is the bias of the estimator \\(\\hat{\\theta}_n\\).\nWe say that \\(\\large\\hat{\\theta}_{n,1}\\) is better than \\(\\large\\hat{\\theta}_{n,2}\\) if : \\[\\Large\\forall n, \\; \\operatorname{MQE}(\\hat{\\theta}_{n,1})\\le \\operatorname{MQE}(\\hat{\\theta}_{n,2})\\]\n\n7 Example :\nLet concider :\n- \\(\\hat{\\theta}_{n,1} = \\operatorname{max}(X_k)\\) and \\(\\hat{\\theta}_{n,4} = \\frac{n+1}{n} \\cdot \\hat{\\theta}_{n,1}\\)\nWe have :\n\nMQE\\((\\hat{\\theta}_{n,1}) = \\dfrac{2\\theta^2}{(n+1)(n+2)}\\)\nMQE\\((\\hat{\\theta}_{n,4}) = \\dfrac{\\theta^2}{n(n+1)}\\)\n\n\\(\\forall n \\geq 2, \\; MQE(\\hat{\\theta}_{n,4}) &lt; MQE(\\hat{\\theta}_{n,1})\\)\nThus, we can conclude that \\(\\hat{\\theta}_{n,4}\\) is better than \\(\\hat{\\theta}_{n,1}\\)\nRemark: \\(\\hat{\\theta}_{n,4}\\) is the best among the two estimators we have considered. Since \\(\\hat{\\theta}_{n,4}\\) is unbiased, we know that for any unbiased estimator \\(\\hat{\\theta}_n\\), we have: \\[ \\Large\\text{Cramer Rao-Bound } \\leq \\mathrm{Var}\\left(\\hat{\\theta}_n\\right)\\]\nIf \\(\\mathrm{Var}\\left(\\hat{\\theta}_{n,4}\\right)\\) equals the Cramer-Rao bound, then the estimator cannot be improved; otherwise, improvement is possible.\n\n\n8 Convergence Illustration in R\n\ntheta &lt;- 2 # assumed true value of the parameter\n# MQE or Variance of the estimators\nmqe1 &lt;- 2 * theta^2 / ((2:50 + 1) * (2:50 + 2))\nmqe4 &lt;- theta^2 / (2:50 * (2:50 + 1))\nplot(mqe4, type = \"l\", col = \"red\")\nlines(mqe1, col = \"blue\")\nlegend(\"topright\",\n       legend = c(\"Estimator 4 (unbiased)\", \"Estimator 1 (max)\"),\n       col = c(\"red\", \"blue\"), lty = 1)\n\n\n\n\n\n\n\n\nThis plot shows that the unbiased estimator \\(\\hat{\\theta}{n,4}\\) consistently outperforms the maximum estimator \\(\\hat{\\theta}{n,1}\\) in terms of MQE, even for relatively small sample sizes (e.g., \\(n = 10\\)). However, as the sample size increases, the MQEs of both estimators get closer, meaning the performance gap narrows — although \\(\\hat{\\theta}_{n,4}\\) remains superior for all \\(n\\).",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean Quadratic Error</span>"
    ]
  },
  {
    "objectID": "The_normal_distribution.html",
    "href": "The_normal_distribution.html",
    "title": "7  The Normal distribution",
    "section": "",
    "text": "7.1 rnorm()\n\\(\\Large X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nPDF of a Normal distribution \\(\\large \\forall t \\in \\mathbb{R}, \\quad\n\\LARGE f_X(t) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(t-\\mu)^2/2\\sigma^2}\\)\nThe rnorm function create a vector of random numbers that follow a ‘bell-shaped’ distribution Parameters: - n the number of random value to generate - mean the center of the distribution (0 by default) - sd the spread of the distriburion (1 by default)\n# normal  distribution with 50 random values, a mean of 0 and a standard deviation of 1 \nrandom_values &lt;- rnorm(50, mean = 0, sd = 1)\nhead(random_values)\n\n[1]  0.01050730  0.27205325  0.05998468  1.21702870 -0.47757773  2.31030220\nRemark:",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal distribution</span>"
    ]
  },
  {
    "objectID": "The_normal_distribution.html#rnorm",
    "href": "The_normal_distribution.html#rnorm",
    "title": "7  The Normal distribution",
    "section": "",
    "text": "“rnorm generates random deviates.”\nIn probability and statistics, a random variate (or simply variate) is a particular outcome or realization of a random variable.\nOther outcomes of the same random variable might yield different values — often referred to as random numbers [@wikipedia-random-variate].",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal distribution</span>"
    ]
  },
  {
    "objectID": "Inverse_Transform_Sampling.html",
    "href": "Inverse_Transform_Sampling.html",
    "title": "8  Inverse Transform Sampling",
    "section": "",
    "text": "8.1 Inverse density function with R\nFrom FSML2 exercice we get the following CFD from the graph below\n\\[\nF_X(t) =\n\\begin{cases}\n0 & \\text{if } t &lt; 0 \\\\\\\\\n\\frac{t^2}{2\\theta^2} & \\text{if } t \\in [0, \\theta] \\\\\\\\\n-\\frac{t^2}{2\\theta^2} + \\frac{2t}{2\\theta} - 1 & \\text{if } t \\in (\\theta, 2\\theta) \\\\\\\\\n1 & \\text{if } t \\geq 2\\theta\n\\end{cases}\n\\]\nThe computation of the inverse function, \\(F_X(t)^{-1}\\) give us :\n\\[\nF_X^{-1} : [0, 1] \\rightarrow [0, 2\\theta]\n\\]\n\\[\nF_X^{-1}(t) =\n\\begin{cases}\n\\sqrt{2\\theta^2 \\cdot t} & \\text{if } t \\in [0, \\tfrac{1}{2}] \\\\\\\\\n2\\theta - \\sqrt{2\\theta^2 \\cdot (1 - t)} & \\text{if } t \\in (\\tfrac{1}{2}, 1]\n\\end{cases}\n\\]\nWhich could be written as a sum with indicator functions as:\n\\[F_X^{-1}(t) = \\sqrt{2\\theta^2 \\cdot t}\\upharpoonleft\\!\\!\\!\\vert_{t \\in[0, \\tfrac{1}{2}]} + 2\\theta - \\sqrt{2\\theta^2 \\cdot (1 - t)} \\upharpoonleft\\!\\!\\!\\vert_{t \\in ]\\tfrac{1}{2}, 1]}\\]\nNote: becareful to count just one time the value \\(tfrac{1}{2}\\)\nThanks to the last equation form, we can write \\(F_X^{-1}(t)\\) in R easily :\nThe logical expressions like (t &lt;= 1/2) and (t &gt; 1/2) act as “indicator functions”.\nIn R, TRUE is treated as 1 and FALSE as 0 in arithmetic operations. This means only the correct formula is applied for each value of t.\nFor example, if t = 0.3, (t &lt;= 1/2) is TRUE (1), so the first formula is used. If t = 0.7, (t &gt; 1/2) is TRUE (1), so the second formula is used.\n# Generate 10,000 random numbers uniformly distributed between 0 and 1\nA &lt;- runif(10000)\n\n# Define the inverse transform function\ninvFX &lt;- function(t, theta) {\n# Logical expressions act as indicators (see explanation above)\n  sqrt(2 * t * theta^2) * (0 &lt;= t) * (t &lt;= 1 / 2) +\n    (2 * theta - sqrt(2 * theta^2 * (1 - t))) * (t &gt; 1 / 2) * (t &lt;= 1)\n}\n\ntheta &lt;- 2  # Set the parameter theta\nX &lt;- invFX(A, theta)   # Apply the inverse transform to the uniform random numbers\nhead(X)  # Display the first few values\n\n[1] 1.990070 1.401274 2.220769 1.621102 1.887976 1.588765\nRemark: In R, you do not need to use ‘return()’ if the value to return is the last line of the function. This is a common style in R, especially for simple functions.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "Inverse_Transform_Sampling.html#display-the-value-of-x",
    "href": "Inverse_Transform_Sampling.html#display-the-value-of-x",
    "title": "8  Inverse Transform Sampling",
    "section": "8.2 Display the value of X",
    "text": "8.2 Display the value of X\n\nhist(X,freq=FALSE)",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "Inverse_Transform_Sampling.html#simulation-of-a-density-function-thanks-to-uniform-random-variable",
    "href": "Inverse_Transform_Sampling.html#simulation-of-a-density-function-thanks-to-uniform-random-variable",
    "title": "8  Inverse Transform Sampling",
    "section": "8.3 Simulation of a density function thanks to uniform random variable",
    "text": "8.3 Simulation of a density function thanks to uniform random variable\n\n“We recognize the function \\(f\\). To generate samples from a random variable \\(X\\) with an unknown density function, it is sufficient to know the inverse of its cumulative distribution function (i.e., \\(F_X^{-1}(t)\\)). By applying this inverse to samples from a uniform distribution, we can simulate values from \\(X\\).”",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "Construction_of_estimators.html",
    "href": "Construction_of_estimators.html",
    "title": "9  Construction of Estimators",
    "section": "",
    "text": "There are two main approaches to constructing estimators in statistics:\n\nThe Method of Moments\nMaximum Likelihood Estimation\n\nThe method of moments is a commonly used and straightforward technique in statistics. It is especially useful when you need to estimate a single parameter associated with a known distribution. The method involves equating sample moments (such as the mean or variance) to their theoretical counterparts and solving for the unknown parameters. Its main advantages are simplicity and broad applicability, making it an accessible introduction to parameter estimation. However, it can be less efficient than other methods and may not always use all the information available in the data.\nMaximum likelihood estimation (MLE) is a more powerful and general approach. It involves finding the parameter values that maximize the likelihood function, i.e., the values that make the observed data most probable under the assumed model. While MLE often requires solving an optimization problem and can be more computationally intensive, it is widely used in practice due to its desirable statistical properties, such as efficiency and consistency, especially as sample size increases. MLE is particularly important in more complex models, including those with multiple parameters.\nWhen we move to linear models, we will encounter the least squares method, which is closely related to maximum likelihood. In fact, for linear regression with normally distributed errors, the least squares estimator is also the maximum likelihood estimator. This connection highlights the central role of MLE in statistical modeling.\n\n\n\n\n\n\nProfessor’s insight\n\n\n\n“The method of moments is the most useful in practice because it is the one that almost everyone knows. Maximum likelihood, while more complex due to its optimization requirements, becomes especially valuable when we deal with models involving several parameters, such as linear models. In those cases, we will see that the least squares method and maximum likelihood are closely related and sometimes even equivalent.”",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Construction of Estimators</span>"
    ]
  },
  {
    "objectID": "Methods_of_Moments.html",
    "href": "Methods_of_Moments.html",
    "title": "10  Method of moments",
    "section": "",
    "text": "10.1 Raw moments:\nLet us consider \\(X_1, \\cdots, X_n\\)  i.i.d random variables whose density depends on an unknown parameter. Consider \\(\\large\\theta\\) a function of this unknown parameter.\nRemark: We specify “a function of this unknown parameter” because, in practice, we may not always be interested in estimating the parameter itself. Instead, we might be interested in estimating a function of it. For example, if \\(X \\sim E(\\lambda)\\), we might be interested in estimating \\(\\theta = 1/\\lambda\\)\nIn the following methods, the \\(k\\) determines the moment we are using:\nLet \\(k\\) be an integer \\(\\ge 1\\) such that there exists a \\(\\large g\\) function with : \\[\\large\\mathbb{E}\\left[X_{1}^k\\right] = g(\\theta)\\]\nNote: the moment of order \\(\\large k\\) is a function of the unknown parameter \\(\\large\\theta\\)\nThen, an estimator \\(\\hat{\\theta}_n\\) for \\(\\theta\\) is the solution of: \\[\\Large g(\\hat{\\theta}_n) = \\frac{1}{n}\\sum_{i=1}^n X_{i}^{k}\\]",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Method of moments</span>"
    ]
  },
  {
    "objectID": "Methods_of_Moments.html#raw-moments",
    "href": "Methods_of_Moments.html#raw-moments",
    "title": "10  Method of moments",
    "section": "",
    "text": "“In constructing estimators using the method of moments, we seek moments that depend on the unknown parameter we wish to estimate.”\n\n\n\n10.1.1 Example\nLet us consider \\(X_1, \\cdots, X_n\\) i.i.d random variables \\(\\sim\\mathcal{U}(\\left[0;\\theta\\right])\\)\n\n“we seek moments that depend on the unknown parameter”:\n\nWe know that since the random variables follow a uniform law defined on \\(\\left[0;\\theta\\right]\\), the expectation is \\(\\dfrac{\\theta}{2}\\) , so we know the first moment \\(k = 1\\).\nTherefore we could apply the method of moments :\n\\(\\mathbb{E}\\left[X_{1}\\right] = \\dfrac{\\theta}{2}= g(\\theta)\\)\nWith \\(g(x) = \\dfrac{x}{2}\\) the function of the unknown parameter.\nHere, we use the notation \\(\\hat{\\theta}_{n,1}\\) to indicate that this estimator is based on the first moment (\\(k=1\\)).\n\\[\ng(\\hat{\\theta}_n) = \\frac{1}{n}\\sum_{i=1}^n X_{i}^{k}\n\\]\n\\[\ng(\\hat{\\theta}_{n,1}) =  \\dfrac{\\hat{\\theta}_{n,1}}{2} = \\frac{1}{n}\\sum_{i=1}^n X_{i} = \\overline{X}_n\n\\]\n\\[\n\\boxed{\\hat{\\theta}_{n,1}=2\\cdot \\overline{X}_n}\n\\]",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Method of moments</span>"
    ]
  },
  {
    "objectID": "Methods_of_Moments.html#centered-moment",
    "href": "Methods_of_Moments.html#centered-moment",
    "title": "10  Method of moments",
    "section": "10.2 Centered Moment",
    "text": "10.2 Centered Moment\nLet \\(k\\) be an integer \\(\\ge 2\\) such that there exists a function \\(h\\) satisfying: \\[\\large\\mathbb{E}\\left[(X_1 - \\mathbb{E}\\left[X_{1}\\right])^k\\right] = h(\\theta)\\] Then, an estimator \\(\\large\\hat{\\theta}_n\\) for \\(\\large\\theta\\) is solution of :\n\\[\\large h(\\hat{\\theta}_n) = \\frac{1}{n}\\sum_{i=1}^n (X_{i}-\\overline{X}_n)^{k}\\] with \\[\\large\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_{i}\\]\nNote: “The use of centered moments in method of moments is rare in practice because centered moments depend on the expectation, which is itself a function of the parameter and must also be estimated.”\nWarning: “These methods allow the construction of estimators for \\(\\theta\\). These estimators are not guaranteed to be unbiased, but they are consistent under general conditions.”\n\n10.2.1 Example\nFollowing the previous example, we could also use the centered moment of order 2, that is to say the second centered moment :\n\\(\\operatorname{Var}(X_1) = \\dfrac{\\theta^2}{12} = h(\\theta)\\)\nwith \\(h(x) = \\frac{x^2}{12}\\)\nAnother estimator \\(\\hat{\\theta}_{n,2}\\) for \\(\\theta\\) is solution of : \\[\nh(\\hat{\\theta}_{n,2}) = \\frac{1}{n}\\sum_{i=1}^n (X_{i}-\\overline{X}_n)^{2}\n\\]\n\\[\n\\dfrac{(\\hat{\\theta}_{n,2})^2}{12} = \\frac{1}{n}\\sum_{i=1}^n (X_{i}-\\overline{X}_n)^{2}\n\\]\nWe obtain :\n\\[\n\\boxed{\\hat{\\theta}_{n,2} =\\sqrt{\\dfrac{12}{n}\\sum_{i=1}^n (X_{i}-\\overline{X}_n)^{2}}}\n\\]",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Method of moments</span>"
    ]
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html",
    "href": "Maximum_Likelihood_Estimation.html",
    "title": "11  Maximum likelihood",
    "section": "",
    "text": "Let us consider \\(X_1, \\cdots, X_n\\)  i.i.d random variables whose density depends on an unknown parameter \\(\\large\\theta\\) .\nIn this context, the likelihood is defined by :\n\\[\n\\Large\n\\mathcal{L}(x_1, \\cdots, x_n;\\, \\theta) =\n\\begin{cases}\n\\displaystyle\\prod_{k=1}^{n} \\mathbb{P}(X_k = x_k) & \\to \\text{discrete case} \\\\[2.5ex]\n\\displaystyle\\prod_{k=1}^{n} f(x_k) & \\to  \\text{continuous case}\n\\end{cases}\n\\]\nAn estimator \\(\\Large\\hat{\\theta}_n\\) for \\(\\Large\\theta\\) is such that :\n\\[\n\\large\\mathcal{L}(x_1, \\cdots, x_n; \\hat{\\theta}_n) = \\max_{a \\in \\mathbb{R}} \\mathcal{L}(x_1, \\cdots, x_n; a)\n\\]",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Maximum likelihood</span>"
    ]
  },
  {
    "objectID": "Motivation.html",
    "href": "Motivation.html",
    "title": "12  Motivation for confidence interval",
    "section": "",
    "text": "Let us consider a Gaussian distribution with mean 0 and variance 2:\n\\[\n  X \\sim \\mathcal{N}(0,2)\n\\]\nSimulating the sample mean of a Gaussian distribution multiple times \nSee the rnorm section\n\nM &lt;- c() # initialize an empty vector to store the means\n\n# Loop over three different sample sizes: 50, 500, and 5,000 \nfor (k in c(50,500,5000)) \n{\n    for (i in 1:50)             # simulate the sample mean 50 times    \n    {\n        A &lt;- rnorm(k,0,sqrt(2)) # generate k observations with mean = 0, variance = 2\n        m &lt;- mean(A)\n        M &lt;- c(M,m)\n    }\n}\n\nWe now create a data frame D with:\n- a column mean containing the 150 sample means (50 for each sample size), and\n- a column label indicating the corresponding sample size.\n\nD &lt;- data.frame(mean = M, label = rep(c('50','500','5000'), each = 50))  # 'rep(..., each = 50)' aligns labels with sample sizes\nboxplot(D$mean ~ D$label,\n        main = \"Distribution of Sample Means by Sample Size\",\n        xlab = \"Sample Size\", ylab = \"Sample Mean\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProfessor’s Insight: Accuracy and Sample Size\n\n\n\nAs we can see in the boxplot:\n- For a sample size of 50, the sample means fluctuate roughly between -0.4 and 0.4. - With 500 observations, the range narrows to around -0.2 to 0.1. - And for 5,000 observations, the fluctuation becomes minimal, between -0.08 and 0.05.\nThis illustrates that as the sample size increases, the accuracy of the sample mean improves, because the fluctuations (i.e., variance) decrease. The quality of the estimation improves at a rate proportional to 1/√n.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Motivation for confidence interval</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "17  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023.\nGgplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org.\n\n\nWikipedia contributors. 2025. “Random Variate —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/wiki/Random_variate. https://en.wikipedia.org/wiki/Random_variate.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "The_normal_distribution.html#sec-rnorm-function",
    "href": "The_normal_distribution.html#sec-rnorm-function",
    "title": "7  The Normal distribution",
    "section": "",
    "text": "“rnorm generates random deviates.”\nIn probability and statistics, a random variate (or simply variate) is a particular outcome or realization of a random variable.\nOther outcomes of the same random variable might yield different values — often referred to as random numbers [@wikipedia-random-variate].",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal distribution</span>"
    ]
  },
  {
    "objectID": "The_normal_distribution.html#references",
    "href": "The_normal_distribution.html#references",
    "title": "7  The Normal distribution",
    "section": "7.3 References",
    "text": "7.3 References\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023.\nGgplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org.\n\n\nWikipedia contributors. 2025. “Random Variate —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/wiki/Random_variate. https://en.wikipedia.org/wiki/Random_variate.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal distribution</span>"
    ]
  },
  {
    "objectID": "Confidence_interval.html",
    "href": "Confidence_interval.html",
    "title": "12  Confidence interval",
    "section": "",
    "text": "12.1 Definition\nThe value of an estimator varies across samples. A confidence interval aims to quantify the uncertainty of this estimation.\nLet us consider:\n- \\(X_1, \\cdots, X_n\\) i.i.d random variables\n- \\(\\theta\\) an unknown parameter\n- \\(\\alpha \\in [0,1]\\), the risk level\nA confidence interval for \\(\\theta\\), with confidence level \\(1−\\alpha\\) (\\(100 \\cdot (1-\\alpha)\\%\\) ) is an interval \\([A_n,B_n]\\) such that the probability that \\(\\theta\\) lies within it is \\(1−\\alpha\\).\n\\(A_n\\) and \\(B_n\\) are functions of the sample data \\(X_1, \\cdots, X_n\\), so that : \\[\n\\large\\mathbb{P}(\\theta \\in [A_n,B_n]) = 1-\\alpha\n\\]",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confidence interval</span>"
    ]
  },
  {
    "objectID": "Confidence_interval.html#motivation",
    "href": "Confidence_interval.html#motivation",
    "title": "12  Confidence interval",
    "section": "12.2 Motivation",
    "text": "12.2 Motivation\nLet us consider a Gaussian distribution with mean 0 and variance 2:\n\\[\n  X \\sim \\mathcal{N}(0,2)\n\\]\nSimulating the sample mean of a Gaussian distribution multiple times\nSee the rnorm section\n\nM &lt;- c() # initialize an empty vector to store the means\n\n# Loop over three different sample sizes: 50, 500, and 5,000 \nfor (k in c(50,500,5000)) \n{\n    for (i in 1:50)             # simulate the sample mean 50 times    \n    {\n        A &lt;- rnorm(k,0,sqrt(2)) # generate k observations with mean = 0, variance = 2\n        m &lt;- mean(A)\n        M &lt;- c(M,m)\n    }\n}\n\nWe now create a data frame D with:\n- a column mean containing the 150 sample means (50 for each sample size), and\n- a column label indicating the corresponding sample size.\n\nD &lt;- data.frame(mean = M, label = rep(c('50','500','5000'), each = 50))  # 'rep(..., each = 50)' aligns labels with sample sizes\nboxplot(D$mean ~ D$label,\n        main = \"Distribution of Sample Means by Sample Size\", # Main title of the boxplot\n        xlab = \"Sample Size\", ylab = \"Sample Mean\") #Label for the x-axis and the the y-axis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProfessor’s Insight: Accuracy and Sample Size\n\n\n\nAs we can see in the boxplot:\n- For a sample size of 50, the sample means fluctuate roughly between -0.4 and 0.4. - With 500 observations, the range narrows to around -0.2 to 0.1. - And for 5,000 observations, the fluctuation becomes minimal, between -0.08 and 0.05.\nThis illustrates that as the sample size increases, the accuracy of the sample mean improves, because the fluctuations (i.e., variance) decrease. The quality of the estimation improves at a rate proportional to 1/√n.\n\n\nEffect of variance on estimation accuracy\nWe now study how variance affects the fluctuation of the sample mean, using the same logic as before but fixing the sample size at 500. We consider three different variances: \\[\nX \\sim \\mathcal{N}(0, 0.1)\n\\] \\[\nX \\sim \\mathcal{N}(0, 1)\n\\] \\[\nX \\sim \\mathcal{N}(0, 10)\n\\]\n\nM &lt;- c() \n\n# Loop over three different variances\nfor (k in c(0.1, 1, 10)) {\n  for (i in 1:50) {\n    A &lt;- rnorm(500, 0, sqrt(k))  \n    m &lt;- mean(A)\n    M &lt;- c(M, m)\n  }\n}\n\nD &lt;- data.frame(mean = M, label = rep(c(\"0.1\", \"1\", \"10\"), each = 50))\n\n# Boxplot of the means by variance group\nboxplot(D$mean ~ D$label,\n        main = \"Distribution of Sample Means by Variance\",\n        xlab = \"Variance\", ylab = \"Sample Mean\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProfessor’s Insight: Sample Size vs Variance\n\n\n\nThese two experiments highlight two distinct effects on the accuracy of the sample mean:\n\nWhen you increase the sample size, you improve the precision of your estimate. The fluctuations around the true mean become smaller, which makes your estimation more reliable.\nIn contrast, when the variance of the distribution increases, the quality of your estimation decreases. The sample mean fluctuates more widely, even with the same number of observations.\n\nSo these two effects move in opposite directions:\n➤ Increasing sample size improves accuracy\n➤ Increasing variance reduces accuracy\n\n\nThese simulations show that the precision of an estimator (like the sample mean) is influenced by two key parameters:\n\nThe sample size \\(n\\): as \\(n\\) increases, the estimation becomes more precise ​\nThe variance \\(\\sigma^2\\) of the underlying distribution: as it increases, the estimate becomes more volatile\n\nSo, the confidence interval, which is used to quantify the uncertainty around an estimate, will depend on both of these factors. Specifically, it depends on a term of the form: \\[\n\\dfrac{\\sigma}{\\sqrt{n}}\n\\]\n\n\n\n\n\n\nProfessor’s Warning: Plug-in Estimators Are Not Harmless\n\n\n\nWhen replacing an unknown parameter (like variance) with an estimator, you apply what’s called a plug-in method. While this is often necessary, it can alter the distribution of your statistic.\nFor example: - If the true variance \\(\\large\\sigma^2\\;\\) were known (which it never is in practice), then the standardized sample mean would follow a normal distribution. - In reality, since \\(\\large\\sigma^2\\;\\) is unknown, we estimate it with the sample variance.\nWhen we do this, the standardized statistic follows a Student’s t-distribution instead.\n\nThis shift in distribution is not trivial — it must be accounted for in confidence intervals.\n\nPractitioners often ignore this and proceed as if nothing changed. But this is not statistically correct:\nEvery plug-in introduces an approximation step, and chaining them carelessly can make your final inference unreliable.\nTo use plug-in methods correctly, you need supporting results (e.g., convergence in distribution).\nWithout them, the theoretical properties of your inference may no longer hold.\n\nIf you only have data, you never have access to the true parameter — only an estimate. Always be aware of how that impacts your conclusions.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confidence interval</span>"
    ]
  },
  {
    "objectID": "The_normal_distribution.html#sec-qnorm-function",
    "href": "The_normal_distribution.html#sec-qnorm-function",
    "title": "7  The Normal distribution",
    "section": "7.2 qnorm()",
    "text": "7.2 qnorm()\nThe qnorm function returns the quantile (e.g. the inverse of the CDF).\nParameters:\n- p vector of probabilities\n- mean the center of the distribution (0 by default)\n- sd standard deviation of the distribution(1 by default)\nqnorm return the value \\(z\\) such that \\(\\mathbb{P}(Z \\le z) = p\\) where \\(Z \\sim \\mathcal{N}(0,1)\\)\n\nalpha &lt;- c(0.01, 0.05, 0.1)\nq &lt;- qnorm(1 - alpha / 2) \nq\n\n[1] 2.575829 1.959964 1.644854\n\n\nThese are the z-values for:\n- 99% CI (α = 0.01)\n- 95% CI (α = 0.05)\n- 90% CI (α = 0.10)\n\n7.2.1 Definition\nLet us consider : \\(\\mathbb{N} \\sim \\mathbfcal{N}(0;1)\\)\nThe quantile of order \\(p\\) for \\(\\mathbb{N}\\), denoted \\(Z_p\\) is given by : \\[\nF_N(Z_p) = p \\Leftrightarrow z_p = F_N^{-1}(p)\n\\]\n\nalpha &lt;- 0.05\np     &lt;- 1 - alpha/2    \nz_p   &lt;- qnorm(p)      \ncurveX &lt;- seq(-3, 3, length.out = 400) # define the plotting window (–3 to 3)\n\nggplot(data.frame(t = curveX), aes(t)) +\n  stat_function(fun = pnorm, linewidth = 1, colour = \"steelblue\") +  # CDF\n  geom_hline(yintercept = p,  linetype = \"dashed\", colour = \"darkgreen\") +\n  geom_vline(xintercept = z_p, linetype = \"dashed\", colour = \"darkgreen\") +\n  annotate(\"text\", x = -2,   y = p + 0.03, label = \"1 - alpha/2\", size = 6, \n            parse = TRUE, colour = \"darkgreen\")+\n  annotate(\"text\", x = z_p, y = -0.05,  label = expression(z[p]), size = 6,\n           colour = \"darkgreen\") +\n  labs(x = \"t\", y = expression(F[N](t))) +\n  theme_minimal(base_size = 14)",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Normal distribution</span>"
    ]
  },
  {
    "objectID": "Confidence_interval.html#confidence-level-effect-of-alpha",
    "href": "Confidence_interval.html#confidence-level-effect-of-alpha",
    "title": "12  Confidence interval",
    "section": "12.3 Confidence level : effect of alpha",
    "text": "12.3 Confidence level : effect of alpha\nSee the qnorm section\n\n# we create a vector of 3 different values for alpha\nalpha &lt;- c(0.01, 0.05, 0.1) \n# compute the z-values associated with the alpha above \nq &lt;- qnorm(1-alpha/2) # will return the quantile, the z-values\nvar &lt;-2\n# we create a data set with a sample of 1000 obs that follow a gaussian law\ndataset &lt;- rnorm(1000,0,sqrt(var)) \n# computation of the lower and upper bound. \nlowerBound &lt;- mean(dataset) - sqrt(var)/sqrt(1000) * q\nupperBound &lt;- mean(dataset) + sqrt(var)/sqrt(1000) * q\nprint(alpha)\n\n[1] 0.01 0.05 0.10\n\nprint(lowerBound)\n\n[1] -0.1940503 -0.1665080 -0.1524158\n\nprint(upperBound)\n\n[1]  0.036338857  0.008796523 -0.005295641\n\n\nFor alpha = 0.01, the confidence interval will be between [-0.076 ; 0.155]. for alpha = 0.1 CI \\(\\in\\) [-0.034;0.113] So, as alpha is increasing, the confidence interval become smaller.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confidence interval</span>"
    ]
  },
  {
    "objectID": "Simple_Linear_Regression.html",
    "href": "Simple_Linear_Regression.html",
    "title": "13  Simple Linear Regression",
    "section": "",
    "text": "13.1 First approach using simulated data in R\n# Generate 100 observations for X uniformly from -7 to 10\nX &lt;- runif(100,-7,10)\n# Generate Y as a linear function of X plus Gaussian noise\nY &lt;- - 4 + 2 * X + rnorm(100,0,2)\n# Plot the scatterplot of X and Y\nplot(X,Y)\nL &lt;- lm(Y~X) # perform a linear model \nplot(X,Y)\n# Add the red regression line to the plot\nabline(L,col=\"red\") # Equivalent to: abline(reg = L, col = 'red')\nThe red line in the plot above represents the fitted simple linear regression model (Y ~ X).\nIt shows the best linear relationship between X and Y based on the simulated data.\nWe generate the data using the theoretical model:\n\\[\nY= -4 + 2 \\times X + \\mathcal{N}(0, 2)\n\\]\nThus, we expect a slope of 2 and an intercept of -4:\nprint(L)\n\n\nCall:\nlm(formula = Y ~ X)\n\nCoefficients:\n(Intercept)            X  \n     -3.960        1.974\nBecause we use observed data with noise, the estimated model does not recover the exact theoretical parameters. Instead, it provides an estimation of the true underlying relationship.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simple_Linear_Regression.html#first-approach-using-simulated-data-in-r",
    "href": "Simple_Linear_Regression.html#first-approach-using-simulated-data-in-r",
    "title": "13  Simple Linear Regression",
    "section": "",
    "text": "The estimated equation is approximately:\n\\[\n\\hat{Y} = -3.686 + 1.962 \\times X\n\\]\n\n\n\n\n\n\n\n\nProfessor’s insight\n\n\n\n“If I wanted a perfect fit, I could always use a high-degree polynomial to match the data exactly. But here’s the issue: when your model fits the data too well, you risk overfitting. It means the model captures not just the underlying trend, but also the random noise in your sample.\nDon’t forget—you’re working with just 100 observations. Even if these data come from the same process, a different sample might produce a very different cloud of points. So if your model is too perfectly tailored to this specific sample, it might not perform well on another one.\nThat’s why you don’t want a model that fits exactly: your goal is to generalize to the entire population, not just to this particular dataset.”",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simple_Linear_Regression.html#sec-lm-function",
    "href": "Simple_Linear_Regression.html#sec-lm-function",
    "title": "13  Simple Linear Regression",
    "section": "13.2 Fitting Linear Models : lm()",
    "text": "13.2 Fitting Linear Models : lm()\nThe lm() function in R is used to fit linear models, including both simple and multiple regression models.\n\nlinearModel &lt;- lm(Y~X)  # Y is the response variable, X is the predictor\n\nThe ~ operator is fundamental in defining linear models in R. An expression of the form Y ~ X specifies that the response variable Y is modeled as a linear function of the predictor(s) X (also called independent variable, explanatory variable, or feature: the variable used to explain or predict the response.)\nThe lm() function returns an object of class “lm”, which contains all the information needed to evaluate and interpret the model, including coefficients, residuals, and diagnostics.\n\nnames(L)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\n\n13.2.1 Coefficients\nSame as print(L) with more precise values to extract the values of the L equation:\n\n L$coefficients \n\n(Intercept)           X \n  -3.960255    1.974269 \n\n\n\n\n13.2.2 Residuals\nFor each observation, we know its true value, denoted by \\(Y\\). When we use the regression model, we obtain a predicted value \\(\\hat{Y}\\).\nThe residual is the difference between the actual value and the predicted value. It is defined as: \\[\n\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i\n\\]\nResiduals measure how far off our model’s predictions are on the training data (also called the learning sample). We talk about residuals when we compare the predicted values \\(\\hat{Y}\\) and the observed values \\(Y\\) for data used to fit the model.\nThe prediction error has the same formula as the residual, but applies to new, unseen data. In that case, we cannot compute it exactly unless we know the true value of the response variable for the new observation.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simple_Linear_Regression.html#prediction-confidence-intervals",
    "href": "Simple_Linear_Regression.html#prediction-confidence-intervals",
    "title": "13  Simple Linear Regression",
    "section": "13.4 Prediction & Confidence Intervals",
    "text": "13.4 Prediction & Confidence Intervals\nWhen we fit a linear model, we may want to know not just the fitted values, but also how certain we are about those predictions. There are two commonly used types of intervals:\n\nConfidence interval: the uncertainty around the mean prediction at a given value of X.\nPrediction interval: the uncertainty around a new individual response at a given X.\n\nBoth can be computed using the predict() function:\n\n\n\n\n\n\nInterpretation Tip\n\n\n\nThe shaded regression line and points from Sample 2 are included only to illustrate sampling variability.\nConfidence and prediction intervals are computed for Sample 1.\n\n\n\n# Prepare sorted data for Sample 1\nX_seq &lt;- df %&gt;%\n  filter(Sample == \"Sample 1\") %&gt;%\n  select(X) %&gt;%\n  arrange(X)\n\n# Compute confidence and prediction intervals\nconf &lt;- predict(mod1, newdata = X_seq, interval = \"confidence\")\npred &lt;- predict(mod1, newdata = X_seq, interval = \"prediction\")\n\n# Combine everything into one data frame\ndf_plot &lt;- cbind(X_seq,\n                 fit = conf[, \"fit\"],\n                 lwr_conf = conf[, \"lwr\"],\n                 upr_conf = conf[, \"upr\"],\n                 lwr_pred = pred[, \"lwr\"],\n                 upr_pred = pred[, \"upr\"])\n\n# Plot regression lines and interval bounds\nggplot() +\n  # Sample 1 points (mapped to color = Sample)\n  geom_point(data = subset(df, Sample == \"Sample 1\"),\n             aes(x = X, y = Y, color = Sample), alpha = 0.7) +\n\n  # Sample 2 points also mapped to color = Sample\n  geom_point(data = subset(df, Sample == \"Sample 2\"),\n             aes(x = X, y = Y, color = Sample), alpha = 0.3) +\n\n  # Regression lines\n  geom_abline(slope = coef(mod1)[\"X\"], intercept = coef(mod1)[\"(Intercept)\"], colour = \"red\") +\n  geom_abline(slope = coef(mod2)[\"X\"], intercept = coef(mod2)[\"(Intercept)\"], colour = \"darkblue\", alpha = 0.3) +\n\n  # Interval bounds\n  geom_line(data = df_plot, aes(x = X, y = lwr_conf), color = \"brown\", linetype = \"dashed\") +\n  geom_line(data = df_plot, aes(x = X, y = upr_conf), color = \"brown\", linetype = \"dashed\") +\n  geom_line(data = df_plot, aes(x = X, y = lwr_pred), color = \"black\", linetype = \"dotted\") +\n  geom_line(data = df_plot, aes(x = X, y = upr_pred), color = \"black\", linetype = \"dotted\") +\n\n  scale_color_manual(values = c(\"Sample 1\" = \"red\", \"Sample 2\" = \"darkblue\")) +\n  labs(title = \"Confidence and Prediction Intervals (Sample 1)\",\n       x = \"X\", y = \"Y\", colour = \"Sample\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nprint(head(df_plot))\n\n          X       fit  lwr_conf  upr_conf  lwr_pred  upr_pred\n1 -6.995939 -18.49928 -19.23810 -17.76046 -22.24641 -14.75215\n2 -6.973301 -18.45285 -19.19025 -17.71544 -22.19970 -14.70599\n3 -6.932878 -18.36993 -19.10482 -17.63504 -22.11629 -14.62357\n4 -6.875320 -18.25186 -18.98317 -17.52056 -21.99752 -14.50621\n5 -6.865959 -18.23266 -18.96339 -17.50194 -21.97820 -14.48712\n6 -6.363672 -17.20234 -17.90207 -16.50261 -20.94196 -13.46272\n\n\nLet’s now interpret the two sets of interval curves shown in the plot:\nThe brown dashed lines represent the 95% confidence interval for the predicted mean \\(\\hat{Y}\\) at each value of \\(X\\).\n→ *“If I fix a value of* \\(X\\), this is where the average predicted \\(\\hat{Y}\\) is likely to fall.”\nThe black dotted lines represent the prediction interval for a new observation \\(Y\\) at each value of \\(X\\).\n→ “If I fix a value of \\(X\\), where might a new observed \\(Y\\) fall?”\nRecall that our model assumes: \\[\nY = a + bX + \\epsilon\n\\] where \\(\\epsilon\\) is a random noise term accounting for variability not explained by the linear model.\nBut the regression line gives us: \\[\n\\hat{Y} = \\hat{a} + \\hat{b}X  \n\\]\nThus:\n\nThe confidence interval (brown) is for \\(\\hat{Y}\\) — the mean response. It assumes the linear model is correct and only accounts for the uncertainty in estimating the coefficients.\n\nThe prediction interval (black) is for \\(Y\\) — an individual new observation. It includes both the uncertainty in the estimated mean \\(\\hat{Y}\\) and the random variability in \\(Y\\) due to the noise term \\(\\epsilon\\), so it’s wider.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simple_Linear_Regression.html#sampling-variability",
    "href": "Simple_Linear_Regression.html#sampling-variability",
    "title": "13  Simple Linear Regression",
    "section": "13.3 Sampling variability",
    "text": "13.3 Sampling variability\nLet’s create two datasets based on the same theoretical model:\n\\[\nY=4+2X+N(0,2)\n\\]\n\n# Set the random number generator's starting point to a fixed value\nset.seed(42) \n\n# First dataset\nX1 &lt;- runif(100, -7, 10)\nY1 &lt;- - 4 + 2 * X1 + rnorm(100, 0, 2)\n\n# Second dataset with the same model\nX2 &lt;- runif(100, -7, 10)\nY2 &lt;- - 4 + 2 * X2 + rnorm(100, 0, 2)\n\n\n# Combine both datasets into a single data frame\ndf &lt;- rbind(\n  data.frame(X = X1, Y = Y1, Sample = \"Sample 1\"),\n  data.frame(X = X2, Y = Y2, Sample = \"Sample 2\")\n)\n\n\nmod1 &lt;- lm(Y ~ X, data = subset(df, Sample == \"Sample 1\"))\nmod2 &lt;- lm(Y ~ X, data = subset(df, Sample == \"Sample 2\"))\n\n\nggplot(data = df, aes(x = X, y = Y, color = Sample)) +\n  geom_point() +\n  geom_abline(slope = coef(mod1)[\"X\"], intercept = coef(mod1)[\"(Intercept)\"], colour = \"red\") +\n  geom_abline(slope = coef(mod2)[\"X\"], intercept = coef(mod2)[\"(Intercept)\"], colour = \"darkblue\") +\n  scale_color_manual(values = c(\"Sample 1\" = \"red\", \"Sample 2\" = \"blue\")) +\n  labs(title = \"Two samples from the same model\",\n       x = \"X\", y = \"Y\", colour = \"Sample\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe figure above shows two samples of 100 observations each, drawn from the same model. Although the data-generating process is identical, the two regression lines (red and dark blue) are not exactly the same. This is a natural consequence of sampling variability: each sample leads to slightly different estimates of the intercept and slope.\n\n\n\n\n\n\nProfessor’s insight\n\n\n\nNotice how the intervals vary across \\(X\\). For each fixed value of \\(X\\), we can ask:\n- “Where is the model likely to predict \\(\\hat{Y}\\)?”\n- “Where might an actual new observation \\(Y\\) fall?”\nUnderstanding the difference between confidence intervals (for the mean prediction) and prediction intervals (for new data points) is essential in interpreting regression results.",
    "crumbs": [
      "Foundations of Statistical Analysis and Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  }
]